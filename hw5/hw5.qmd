---
title: "Biostat 212a Homework 5"
subtitle: "Due Mar 16, 2024 @ 11:59PM"
author: "Li Zhang 206305918"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r}
sessionInfo()
```

```{r}
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)
library(magrittr)

library(kernlab)

```


## ISL Exercise 9.7.1 (10pts)

(a)
```{r}
X1 <- seq(-10, 10, by = 0.5)
X2 <- seq(-10, 10, by = 0.5)

grid <- expand.grid(X1, X2)

grid$X2_hyperplane1 <- 1 + 3 * grid$Var1 - grid$Var2
grid$X2_hyperplane2 <- (-2 + grid$Var1 + 2 * grid$Var2) / 2

# Plot the hyperplane
plot(grid$Var1, grid$Var2, type = "n", xlab = "X1", ylab = "X2", 
     main = "Hyperplane Sketch", frame.plot = FALSE)
abline(h = 0, v = 0, col = "gray") 
contour(X1, X2, matrix(grid$X2_hyperplane1, length(X1), length(X2)), 
        levels = 0, add = TRUE, col = "blue")
contour(X1, X2, matrix(grid$X2_hyperplane2, length(X1), length(X2)), 
        levels = 0, add = TRUE, col = "red")
```
The blue line represents the hyperplane defined by $1 + 3X_1 - X_2 = 0$

The red line represents the hyperplane defined by $-2 + X_1 + 2X_2 = 0$.

(a)

The set of points for which $1 + 3X_1 - X_2 > 0$ is the set of points **above** the blue hyperplane.

The set of points for which $1 + 3X_1 - X_2 < 0$ is the set of points **below** the blue hyperplane.

(b)

The set of points for which $-2 + X_1 + 2X_2 > 0$ is the set of points **above** the red hyperplane.

The set of points for which $-2 + X_1 + 2X_2 < 0$ is the set of points **below** the red hyperplane.


## ISL Exercise 9.7.2 (10pts)

(a)


```{r}
center <- c(-1, sqrt(2))
radius <- 2

theta <- seq(0, 2 * pi, length.out = 100)

x <- center[1] + radius * cos(theta)
y <- center[2] + radius * sin(theta)

plot(x, y, type = "l", asp = 1, xlab = "X", ylab = "Y", main = "(1 + X_1)^2 + (2 - X_2)^2 = 4")
polygon(x, y, col = "lightblue")
abline(h = 0, v = 0, col = "gray") 
```

(b)

The set of points for which $(1 + X_1)^2 + (2 - X_2)^2 <= 4$ is the set of points in the blue circle(boundary included).

The set of points for which $(1 + X_1)^2 + (2 - X_2)^2 > 4$ is the set of points outside the blue circle.

(c)

(0,0) is classified as the blue class.

(-1,1): red

(2,2): blue

(3,8): blue

(d)

The decision boundary: $(1 + X_1)^2 + (2 - X_2)^2 = 4$

So, it is not linear in terms of $X_1$ and $X_2$.

but if we expand the equation, we get $X_1^2 + 2X_1 + X_2^2 - 4X_2 = 0$, 

then it is linear in terms of $X_1$, $X_2$, $X_1^2$, and $X_2^2$.


## Support vector machines (SVMs) on the `Carseats` data set (30pts)

Follow the machine learning workflow to train support vector classifier (same as SVM with linear kernel), SVM with polynomial kernel (tune the degree and regularization parameter $C$), and SVM with radial kernel (tune the scale parameter $\gamma$ and regularization parameter $C$) for classifying `Sales<=8` versus `Sales>8`. Use the same seed as in your HW4 for the initial test/train split and compare the final test AUC and accuracy to those methods you tried in HW4.


### Careseats dataset

```{r}
str(Carseats)
```

```{r}
Carseats$Sales <- ifelse(Carseats$Sales <= 8, "0", "1")
Carseats$Sales <- factor(Carseats$Sales)
```
```{r}
Carseats <- na.omit(Carseats)
Carseats %>% tbl_summary(by = Sales)
```



- The goal is to predict `Sales <= 8` versus `Sales > 8`

### Initial split into test and non-test sets

We randomly split the data in half of test data and another half of non-test data. Stratify on `AHD`.

```{r}
# For reproducibility
set.seed(210)

data_split <- initial_split(
  Carseats, 
  prop = 0.5,
  strata = Sales
  )
data_split

Carseats_other <- training(data_split)
dim(Carseats_other)

Carseats_test <- testing(data_split)
dim(Carseats_test)
```
### train support vector classifier (same as SVM with linear kernel)


### SVM with polynomial kernel (tune the degree and regularization parameter $C$)
#### Recipe
```{r}
svm_recipe <- 
  recipe(
    Sales ~ ., 
    data = Carseats_other
  ) %>%

  # create traditional dummy variables (necessary for svm)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Carseats_other, retain = TRUE)
svm_recipe
```
#### Model
```{R}
svm_mod <- 
  svm_poly(
    mode = "classification",
    cost = tune(),
    degree = tune(),
    # scale_factor = tune()
  ) %>% 
  set_engine("kernlab")
svm_mod
```
#### Workflow
```{r}
svm_wf <- workflow() %>%
  add_recipe(svm_recipe) %>%
  add_model(svm_mod)
svm_wf
```
#### Tuning grid

```{r}
param_grid <- grid_regular(
  cost(range = c(-3, 2)),
  degree(range = c(1, 5)),
  #scale_factor(range = c(-1, 1)),
  levels = c(5)
  )
param_grid
```

#### CV

Set cross-validation partitions.
```{r}
set.seed(212)

folds <- vfold_cv(Carseats_other, v = 5)
folds
```

Fit cross-validation.

```{r}
svm_fit <- svm_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
svm_fit
```

Visualize CV results:

```{r}
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = degree, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()
```
Show the top 5 models.

```{r}
svm_fit %>%
  show_best("roc_auc")
```
Let’s select the best model.

```{r}
best_svm <- svm_fit %>%
  select_best("roc_auc")
best_svm
```
#### Finalize the model

```{r}
# Final workflow
final_wf <- svm_wf %>%
  finalize_workflow(best_svm)
final_wf
```
```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```


### SVM with radial kernel (tune the scale parameter $\gamma$ and regularization parameter $C$)

#### Model

```{R}
svm_mod <- 
  svm_rbf(
    mode = "classification",
    cost = tune(),
    rbf_sigma = tune()
  ) %>% 
  set_engine("kernlab")
svm_mod
```

#### Workflow
```{r}
svm_wf <- workflow() %>%
  add_recipe(svm_recipe) %>%
  add_model(svm_mod)
svm_wf
```

#### Tuning grid

```{r}
param_grid <- grid_regular(
  cost(range = c(-8, 5)),
  rbf_sigma(range = c(-5, -3)),
  levels = c(14, 5)
  )
param_grid
```

#### CV

Set cross-validation partitions.
```{r}
set.seed(212)

folds <- vfold_cv(Carseats_other, v = 5)
folds
```

Fit cross-validation.

```{r}
svm_fit <- svm_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
svm_fit
```

Visualize CV results:

```{r}
svm_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc" ) %>%
  ggplot(mapping = aes(x = cost, y = mean, alpha = rbf_sigma)) +
  geom_point() +
  geom_line(aes(group = rbf_sigma)) +
  labs(x = "Cost", y = "CV AUC") +
  scale_x_log10()
```

Show the top 5 models.

```{r}
svm_fit %>%
  show_best("roc_auc")
```

Let’s select the best model.

```{r}
best_svm <- svm_fit %>%
  select_best("roc_auc")
best_svm
```

#### Finalize the model

```{r}
# Final workflow
final_wf <- svm_wf %>%
  finalize_workflow(best_svm)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```





## Bonus (10pts)

Let
$$
f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = \beta_0 + \beta^T X. 
$$
Then $f(X)=0$ defines a hyperplane in $\mathbb{R}^p$. Show that $f(x)$ is proportional to the signed distance of a point $x$ to the hyperplane $f(X) = 0$. 
