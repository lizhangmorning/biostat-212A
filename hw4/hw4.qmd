---
title: "Biostat 212a Homework 4"
subtitle: "Due Mar. 5, 2024 @ 11:59PM"
author: "Li Zhang 206305918"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---
```{r}
sessionInfo()
```

```{r}
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)
```
## ISL Exercise 8.4.3 (10pts)
```{r}
# Generate sequence of pˆm1 values
p_m1 <- seq(0, 1, by = 0.01)

# Calculate corresponding values of Gini index, classification error, and entropy
gini <- 2 * p_m1 * (1 - p_m1)
classification_error <- 1 - pmax(p_m1, 1 - p_m1)
entropy <- -p_m1 * log2(p_m1) - (1 - p_m1) * log2(1 - p_m1)

# Plot
plot(p_m1, gini, type = "l", col = "red", ylim = c(0, 1), xlab = "pˆm1", ylab = "Value", main = "Gini Index, Classification Error, and Entropy")
lines(p_m1, classification_error, type = "l", col = "blue")
lines(p_m1, entropy, type = "l", col = "green")
legend("topright", legend = c("Gini Index", "Classification Error", "Entropy"), col = c("red", "blue", "green"), lty = 1)
```


## ISL Exercise 8.4.4 (10pts)

(a)

Here is my tree:

![](Q8.4.4a.pdf)

(b)

Here is my diagram:

![](Q8.4.4b.pdf)


## ISL Exercise 8.4.5 (10pts)
```{r}
p = c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
sum(p >= 0.5) > sum(p < 0.5)
```
So, the majority class is `p >= 0.5` and the final classification is **red**.


```{r}
mean(p)
mean(p) > 0.5
```
So, the average probability is 0.49 and the final classification is **green**.

## ISL Lab 8.3. `Boston` data set (30pts)

Follow the machine learning workflow to train regression tree, random forest, and boosting methods for predicting `medv`. Evaluate out-of-sample performance on a test set.



### Boston dataset

```{r}
Boston %>% tbl_summary()
```

```{r}
Boston <- Boston %>% filter(!is.na(medv))
```

### Initial split into test and non-test sets

We randomly split the data in half of test data and another half of non-test data.

```{r}
# For reproducibility
set.seed(203)

data_split <- initial_split(
  Boston, 
  prop = 0.5
  )
data_split

Boston_other <- training(data_split)
dim(Boston_other)

Boston_test <- testing(data_split)
dim(Boston_test)
```


### Regression tree
#### Recipe

```{r}
tree_recipe <- 
  recipe(
    medv ~ ., 
    data = Boston_other
  ) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal()) %>%
  step_naomit(medv) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Boston_other, retain = TRUE)
tree_recipe
```
#### Model

```{r}
regtree_mod <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5,
  mode = "regression",
  engine = "rpart"
  ) 

```

#### Workflow

Here we bundle the recipe and model.

```{r}
tree_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(regtree_mod)
tree_wf
```
#### Tuning grid

`ccp_alpha` is the Minimal Cost-Complexity Pruning parameter. Greater values of `ccp_alpha` increase the number of nodes pruned.

```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(100, 5))
```

#### Cross-validation (CV)

Set cross-validation partitions.
```{r}
set.seed(203)

folds <- vfold_cv(Boston_other, v = 5)
folds
```
Fit cross-validation.
```{r}
tree_fit <- tree_wf %>%
  tune_grid(
    resamples = folds,
    grid = tree_grid,
    metrics = metric_set(rmse, rsq)
    )
tree_fit
```
Visualize CV results:
```{r}
tree_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = cost_complexity, y = mean, color = tree_depth)) +
  geom_point() + 
  geom_line() + 
  labs(x = "cost_complexity", y = "CV mse")
```


#### Finalize our model

Now we are done tuning. Finally, let’s fit this final model to the whole training data and use our test data to estimate the model performance we expect to see with new data.

```{r}
tree_fit %>%
  show_best("rmse")
```
Let's select the best model.
```{r}
best_tree <- tree_fit %>%
  select_best("rmse")
best_tree
```
```{r}
# Final workflow
final_wf <- tree_wf %>%
  finalize_workflow(best_tree)
final_wf
```
```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

#### Visualize the final model
```{r}
library(rpart.plot)
final_tree <- extract_workflow(final_fit)
final_tree

final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```

```{r}
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```
### Random forest

#### Recipe

```{r}
rf_recipe <- 
  recipe(
    medv ~ ., 
    data = Boston_other
  ) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal()) %>%
  step_naomit(medv) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Boston_other, retain = TRUE)
rf_recipe
```

#### Model

```{r}
rf_mod <- 
  rand_forest(
    mode = "regression",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_mod
```

#### Workflow
```{r}
rf_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_mod)
rf_wf
```

#### Tuning grid

In general, it's not necessary to tune a random forest. Using the default of `n_estimators=100` and `max_features=1.0` (bagging) or `max_features='sqrt'` works well.

Here we tune the number of trees `n_estimators` and the number of features to use in each split `max_features`.

Here we tune the number of trees `trees` and the number of features to use in each split `mtry`.

```{r}
param_grid <- grid_regular(
  trees(range = c(100L, 300L)), 
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
  )
param_grid
```


#### Cross-validation (CV)

Set cross-validation partitions.
```{r}
set.seed(203)

folds <- vfold_cv(Boston_other, v = 5)
folds
```

Fit cross-validation.
```{r}
rf_fit <- rf_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(rmse, rsq)
    )
rf_fit
```

Visualize CV results:
```{r}
rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  # geom_point() + 
  geom_line() + 
  labs(x = "Num. of Trees", y = "CV mse")
```

Show the top 5 models.
```{r}
rf_fit %>%
  show_best("rmse")
```

Let's select the best model.
```{r}
best_rf <- rf_fit %>%
  select_best("rmse")
best_rf
```


#### Finalize our model

Now we are done tuning. Finally, let’s fit this final model to the whole training data and use our test data to estimate the model performance we expect to see with new data.

```{r}
# Final workflow
final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

### Boosting

```{r}
Boston %>% tbl_summary()
```

```{r}
Boston <- Boston %>% filter(!is.na(medv)) %>%
  select(medv, crim, zn, indus,
         nox, rm, age, dis, tax,
         ptratio, lstat)
```
 
#### Recipe

```{r}
gb_recipe <- 
  recipe(
    medv ~ ., 
    data = Boston_other
  ) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal()) %>%
  step_naomit(medv) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Boston_other, retain = TRUE)
gb_recipe
```

#### Model


```{r}
gb_mod <- 
  boost_tree(
    mode = "regression",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod
```


#### Workflow

Here we bundle the recipeand model.

```{r}
gb_wf <- workflow() %>%
  add_recipe(gb_recipe) %>%
  add_model(gb_mod)
gb_wf
```

#### Tuning grid

Here we tune the number of trees `n_estimators` and the learning rate `learning_rate`.

Here we tune the number of trees `trees` and the number of features to use in each split `mtry`.

```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 4L)),
  learn_rate(range = c(-3, -0.5), trans = log10_trans()),
  levels = c(4, 10)
  )
param_grid
```

#### Cross-validation (CV)

Set cross-validation partitions.
```{r}
set.seed(203)

folds <- vfold_cv(Boston_other, v = 5)
folds
```

Fit cross-validation.
```{r}
library(xgboost)
gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(rmse, rsq)
    )
gb_fit
```

Visualize CV results:
```{r}
gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  geom_line() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```

Show the top 5 models.
```{r}
gb_fit %>%
  show_best("rmse")
```
Let's select the best model.
```{r}
best_gb <- gb_fit %>%
  select_best("rmse")
best_gb
```

#### Finalize our model

Now we are done tuning. Finally, let’s fit this final model to the whole training data and use our test data to estimate the model performance we expect to see with new data.

```{r}
# Final workflow
final_wf <- gb_wf %>%
  finalize_workflow(best_gb)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

#### Visualize the final model
```{r}
#library(rpart.plot)
final_tree <- extract_workflow(final_fit)
final_tree
```

```{r}
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```


### Summary

- I would choose **random forest** for predicting `medv`, because it has the lowest test rmse and highest test rsq.


## ISL Lab 8.3 `Carseats` data set (30pts)

Follow the machine learning workflow to train classification tree, random forest, and boosting methods for classifying `Sales <= 8` versus `Sales > 8`. Evaluate out-of-sample performance on a test set.


### Careseats dataset

```{r}
str(Carseats)
```

```{r}
Carseats$Sales <- ifelse(Carseats$Sales <= 8, "0", "1")
Carseats$Sales <- factor(Carseats$Sales)
```
```{r}
Carseats <- na.omit(Carseats)
Carseats %>% tbl_summary()
```

- The goal is to predict `Sales <= 8` versus `Sales > 8`

### Initial split into test and non-test sets

We randomly split the data in half of test data and another half of non-test data. Stratify on `AHD`.

```{r}
# For reproducibility
set.seed(210)

data_split <- initial_split(
  Carseats, 
  prop = 0.5,
  strata = Sales
  )
data_split

Carseats_other <- training(data_split)
dim(Carseats_other)

Carseats_test <- testing(data_split)
dim(Carseats_test)
```
### Classification tree

#### Recipe

```{r}
tree_recipe <- 
  recipe(
    Sales ~ ., 
    data = Carseats_other
  ) %>%
  step_naomit(all_predictors()) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Carseats_other, retain = TRUE)
tree_recipe
```

#### Model

```{r}
classtree_mod <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5,
  mode = "classification",
  engine = "rpart"
  ) 
```

#### Workflow
```{r}
tree_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(classtree_mod) 
tree_wf
```

#### Tuning grid
```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(100,5))
```

#### Cross-validation (CV)
```{r}
set.seed(212)

folds <- vfold_cv(Carseats_other, v = 5)
folds
```

Fit cross-validation.
```{r}
tree_fit <- tree_wf %>%
  tune_grid(
    resamples = folds,
    grid = tree_grid,
    metrics = metric_set(accuracy, roc_auc)
    )
tree_fit
```

Visualize CV results:
```{r}
tree_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = cost_complexity, y = mean, color = tree_depth)) +
  geom_point() + 
  geom_line() + 
  labs(x = "cost_complexity", y = "CV ROC AUC", color = "tree_depth") 
```
#### Finalize our model

Now we are done tuning. Finally, let’s fit this final model to the whole training data and use our test data to estimate the model performance we expect to see with new data.

```{r}
tree_fit %>%
  show_best("roc_auc")
```
Let's select the best model.
```{r}
best_tree <- tree_fit %>%
  select_best("roc_auc")
best_tree
```

```{r}
# Final workflow
final_wf <- tree_wf %>%
  finalize_workflow(best_tree)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```
#### Visualize the final model
```{r}
library(rpart.plot)
final_tree <- extract_workflow(final_fit)
final_tree

final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```

```{r}
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```

### Random forest

#### Recipe

```{r}
rf_recipe <- 
  recipe(
    Sales ~ ., 
    data = Carseats_other
  ) %>%
  step_zv(all_numeric_predictors()) %>% 
  prep(training = Carseats_other, retain = TRUE)
rf_recipe
```

#### Model

```{r}
rf_mod <- 
  rand_forest(
    mode = "classification",
    mtry = tune(),
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_mod
```
#### Workflow
```{r}
rf_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_mod)
rf_wf
```
#### Tuning grid
```{r}
param_grid <- grid_regular(
  mtry(range = c(1L, 5L)),
  trees(range = c(100L, 300L)),
  levels = c(3, 5)
  )
param_grid
```
#### Cross-validation (CV)
```{r}
set.seed(212)

folds <- vfold_cv(Carseats_other, v = 5)
folds
```

Fit cross-validation.
```{r}
rf_fit <- rf_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(accuracy, roc_auc)
    )
rf_fit
```

Visualize CV results:
```{r}
rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  geom_point() + 
  geom_line() + 
  labs(x = "Num. of Trees", y = "CV ROC AUC")
```

Show the top 5 models.
```{r}
rf_fit %>%
  show_best("roc_auc")
```

Let's select the best model.
```{r}
best_rf <- rf_fit %>%
  select_best("roc_auc")
best_rf
```

#### Finalize our model

Now we are done tuning. Finally, let’s fit this final model to the whole training data and use our test data to estimate the model performance we expect to see with new data.

```{r}
# Final workflow
final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

### Boosting

#### Recipe

```{r}
gb_recipe <- 
  recipe(
    Sales ~ ., 
    data = Carseats_other
  ) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>% 
  prep(training = Carseats_other, retain = TRUE)
gb_recipe
```

#### Model

```{r}
gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod
```

#### Workflow

Here we bundle the recipe and model.

```{r}
gb_wf <- workflow() %>%
  add_recipe(gb_recipe) %>%
  add_model(gb_mod)
gb_wf
```

#### Tuning grid

Here we tune the number of trees `n_estimators` and the learning rate `learning_rate`.

Here we tune the number of trees `trees` and the number of features to use in each split `mtry`.

```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10)
  )
param_grid
```

#### Cross-validation (CV)

Set cross-validation partitions.
```{r}
set.seed(212)

folds <- vfold_cv(Carseats_other, v = 5)
folds
```

Fit cross-validation.
```{r}
gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(accuracy, roc_auc)
    )
gb_fit
```

Visualize CV results:
```{r}
gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  geom_line() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```

Show the top 5 models.
```{r}
gb_fit %>%
  show_best("roc_auc")
```

Let's select the best model.
```{r}
best_gb <- gb_fit %>%
  select_best("roc_auc")
best_gb
```

#### Finalize our model

Now we are done tuning. Finally, let’s fit this final model to the whole training data and use our test data to estimate the model performance we expect to see with new data.

```{r}
# Final workflow
final_wf <- gb_wf %>%
  finalize_workflow(best_gb)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

#### Summary

- I would choose **boosting** for classifying `Sales <= 8` versus `Sales > 8`, because it has the highest test roc_auc and accuracy.